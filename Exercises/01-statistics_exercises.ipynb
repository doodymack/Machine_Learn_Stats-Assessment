{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paul Mc Grath - Machine Learning & Stats- Winter 2022 Module- Assessment  \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics- Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scientific research, the null hypothesis (often denoted H0) is the claim that no difference or relationship exists between two sets of data or variables being analyzed. The null hypothesis is that any experimentally observed difference is due to *chance alone*, and an underlying causative relationship does not exist, hence the term \"null\". In addition to the null hypothesis, an alternative hypothesis (often denoted HA or H1) is also developed, which claims that *a relationship does exist between two variables*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statement being tested in a test of statistical significance is called the null hypothesis. The test of significance is designed to assess the strength of the evidence against the null hypothesis. Usually, the null hypothesis is a statement of *'no effect' or 'no difference 'H0'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Statistical significance test:\n",
    "\n",
    "Take a random sample from the population.\n",
    "\n",
    "-  If the sample data are consistent with the null hypothesis, then: do not reject the null hypothesis.\n",
    "-  If the sample data are inconsistent with the null hypothesis, then reject the null hypothesis and conclude that the alternative hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contingency Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, a contingency table is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables.\n",
    "Contingency tables classify outcomes for one variable in rows and the other in columns.\n",
    "The values at the row and column intersections are frequencies for each unique combination of the two variables."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An odds ratio (OR) is a statistic that quantifies the strength of the association between two events, A and B. The odds ratio is defined as the ratio of the odds of A in the presence of B and the odds of A in the absence of B, or equivalently (due to symmetry), the ratio of the odds of B in the presence of A and the odds of B in the absence of A. Two events are independent if and only if the OR equals 1, i.e., the odds of one event are the same in either the presence or absence of the other event. If the OR is greater than 1, then A and B are associated (correlated) in the sense that, compared to the absence of B, the presence of B raises the odds of A, and symmetrically the presence of A raises the odds of B. Conversely, if the OR is less than 1, then A and B are negatively correlated, and the presence of one event reduces the odds of the other event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the true odds ratio of the populations underlying the observations is one, and the observations were sampled from these populations under a condition: the marginals of the resulting table must equal those of the observed table. The statistic returned is the unconditional maximum likelihood estimate of the odds ratio, and the p-value is the probability under the null hypothesis of obtaining a table at least as extreme as the one that was actually observed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the necessary prerequisites for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'%.3f'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set outputs to 3 decimal places\n",
    "%precision %.3f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required libraries\n",
    "\n",
    "# Efficient numerical arrays.\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Alternative statistics package.\n",
    "import statsmodels.stats.weightstats as stat\n",
    "\n",
    "# Mains statistics package.\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fancier plotting.\n",
    "import seaborn as sns\n",
    "\n",
    "# Better sized plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Nicer colours and styles for plots.\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher Tea Test Background\n",
    "\n",
    "The Lady Tasting Tea problem was first described by <a href=\"http://www-history.mcs.st-andrews.ac.uk/Biographies/Fisher.html\" style=\"color: #ff791e\">Ronald A. Fisher</a> in his book The Design of Experiments.\n",
    "\n",
    "<img src=\"https://www.sciencehistory.org/sites/default/files/styles/rte_full_width/public/rte/fisher_as_young_man.jpg\" style=\"border: 1px solid #ff791e\" width=\"200px\">\n",
    "\n",
    "[Fisher Tea Test Background](https://www.sciencehistory.org/distillations/ronald-fisher-a-bad-cup-of-tea-and-the-birth-of-modern-statistics \"Ronald Fisher, a Bad Cup of Tea, and the Birth of Modern Statistics\") <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher described the tea tasting problem as follows.\n",
    "\n",
    "> A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup: We will consider the problem of designing an experiment by means of which this assertion can be tested. For this purpose let us first lay down a simple form of experiment with a view to studying its limitations and its characteristics, both those which appear to be essential to the experimental method, when well developed, and those which are not essential but auxiliary.\n",
    "\n",
    "> Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received.\n",
    "\n",
    "> In the book, Fisher describes the *null hypothesis*. <u>It is the statement to be countered by the experiment.</u>  \n",
    "In this case it is the hypothesis that the subject cannot tell the difference between a cup of tea that had milk in it first and one that had the tea in it first.  \n",
    "<br>\n",
    "**Null Hypothesis:** <u>the subject can not tell if the cup had milk in it first or last.</u>  \n",
    "Once we collect the experimental data we evaluate how likely we were to see such data if the null hypothesis is true. If it is very unlikely, then we may reject the null hypothesis. Typically we have an alternative hypothesis that we suggest rejecting the null hypothesis is evidence for.  \n",
    "<br>\n",
    "**Alternative Hypothesis:** <u>the subject can tell.</u>\n",
    "<br>Should the subject pick the correct four cups with milk in them first, there is only a 1 in 70 (~1.4%) chance of them having done that if they were simply guessing. Fisher considered that unlikely enough to reject the null hypothesis if they manage it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of a null hypothesis is used differently in two approaches to statistical inference. In the significance testing approach of Ronald Fisher, a null hypothesis is rejected if the observed data are significantly unlikely to have occurred if the null hypothesis were true: *the number of cups that were correctly chosen is by chance alone*.  \n",
    "In this case, the null hypothesis is rejected and an alternative hypothesis is accepted in its place: *the number of cups that were correctly chosen is more than predicted by chance alone*.  <br> <br>In summary: The hypothesis that chance alone is responsible for the results of the experiment is called the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us investigate the number of ways in which the lady can arrange the eight cups into two groups of four.\n",
    "- The first group is the cups with milk first.\n",
    "- The second group is the cups with tea first.\n",
    "- Once we select four cups for the first group, the second group is made up of the remaining cups.\n",
    "- So, we only really need to count how many different ways there are to select four cups from eight.\n",
    "- To begin we give each of the eight cups a label from 0 to 7.\n",
    "- Note that the cups have no order in the experiment, we are labelling them with integers simply for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Python code for the eight cups.\n",
    "cups = list(range(8))\n",
    "cups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can assume the subject is trying to pick the four cups with milk in first.<br>\n",
    "- If the subject is picking the cups randomly then they are randomly picking one of seventy different options.<br>\n",
    "- By randomly, we mean that they can not really tell which cups have the milk in first.<br>\n",
    "- Therefore they have only a probability of 1/70 of getting the right answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of ways of selecting four things from eight where the order doesnt matter  \n",
    "**(8*7*6*5)/(4*3*2*1)**\n",
    "\n",
    "8 cups randomly labelled: 0 1 2 3 4 5 6 7.  \n",
    "4 spots:  _ _ _ _.  Select the 1st cup with tea in them.  \n",
    "For example say 1st cup chosen is 2, then 0, then 7 then 5. \n",
    "1st 4 spots: 2 0 7 5.  \n",
    "1st spot have 8 selection possibilities.  \n",
    "Thereafter 2nd spot have 7 selection possibilities.  \n",
    "3rd spot: have 6 selection possibilities.  \n",
    "4th spot: have 5 selection possibilities and so on.\n",
    "Therefore there are (8x7x6x5) ways to choose/possibilities.  \n",
    "The order within group is not important.  Just that the cup is in the right group.  \n",
    "i.e.  i.e. 2-0-7-5 same as 0-2-7-5 (these are in the 4 cups of Group 1 (tea first)).  \n",
    "Therefore we have to take in to account how many times have we double counted the same combination.      \n",
    "We now need to remove the potential duplicates. Dividing (8x7x6x5) by(4x3x2x1) does this.  \n",
    "This results in 70 possibilities to select the cups at random.\n",
    "Just one of these will get the 4 right cups in Group 1.  \n",
    "It works out that there is a 1/70 chance or 1.4% the taster will get the 4 'correct' cups right by chance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Itertools\n",
    "Itertools is a package in python that returns the *distict number* of ways of selecting *n* unordered items from a list of elements/numbers *n*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2, 3),\n",
       " (0, 1, 2, 4),\n",
       " (0, 1, 2, 5),\n",
       " (0, 1, 2, 6),\n",
       " (0, 1, 2, 7),\n",
       " (0, 1, 3, 4),\n",
       " (0, 1, 3, 5),\n",
       " (0, 1, 3, 6),\n",
       " (0, 1, 3, 7),\n",
       " (0, 1, 4, 5)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use of itertools function from python. \n",
    "# Itertools returns the number of distinct Returns 70 rows (70 combinations) for 8 cups declared above\n",
    "# Note 1:  index goes from 0-7\n",
    "# Note 2: Itertools starts below with 0,1,2,3 but this is simply the computer way of displaying a list of integers.  \n",
    "# If this is removed there is no reason to start with 0,1,2,3.etc\n",
    "poss = list(itertools.combinations(cups, 4))\n",
    "poss[0:10]\n",
    "# poss[0:70] returns all combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only one of the 70 randomly selected possibilities is the desired result.\n",
    "# i.e. the cups chosen are the correct 4 cups (milk first or tea first)\n",
    "# Thus 1 in 70 chance or:\n",
    "1/70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen above that with 8 cups of tea the probability of randomly selecting the correct cups is equal to 0.014 or **1.4%**.  \n",
    "We can use the maths library from python to try to find the minimum number of cups of tea required to ensure the probability of randomly selecting the correct cups is less than or equal to **1%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to itertools we can do this with the help of Python Maths libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ways of selecting 4 cups from 8 (number of choices)\n",
    "math.comb(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 4 cups from 8 is 1/(number of choices)\n",
    "1.0/math.comb(8,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 4 cups from 8 - as a percentage\n",
    "1.0/math.comb(8,4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. for the tea experiment with 8 cups - 4 tea first, 4 milk first the probability of selecting a given combination is approx 1.5%  as was quoted above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statisitics Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise 1</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "#### Calculate the minimum number of cups of tea required to ensure the probability of randomly selecting the correct cups is less than or equal to 1%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of combinations from selecting correct 5 cups from 10 is:\n",
    "math.comb(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of randonly selecting correct 5 cups from 10 is:\n",
    "1.0/math.comb(10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.397"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability as a percentage:\n",
    "1.0/math.comb(10,5)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if the experiment was done with 10 cups total, five with milk in first and five with tea in first, <br> then the chance of selecting correctly randomly is **much** less than 1%.<br>\n",
    "Lets try 9 cups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ways of selecting 4 cups from 9\n",
    "math.comb (9,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.794"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of randonly selecting correct 4 (or 5) cups from 9 -as a percentage is:\n",
    "1.0/math.comb(9,4)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thus choosing 4 cups with milk first from 10 cups the chances of correct selections based on random selection is 0.004 or circa **0.4%**\n",
    "\n",
    "##### Choosing 4 cups with milk first from 9 cups the chances of correct selections based on random selection is 0.00794 or circa **0.8%**.\n",
    "\n",
    "#### Answer is 9 cups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### *Bonus:* How many would be required if you were to let the taster get one cup wrong while maintaining the 1% threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets re-look at the original experiment from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ways of selecting 3 cups from 8 (number of choices)\n",
    "math.comb(8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.018"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 8 is 1/(number of choices)\n",
    "1.0/math.comb(8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.786"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 8 - as a percentage\n",
    "1.0/math.comb(8,3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thus the probability of selecting 3 correct from the 8 cups is >1%.  We need to add more 'random' cups.  Lets look at using a set of 9 cups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ways of selecting 3 cups from 9 (number of choices)\n",
    "math.comb(9,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 9 is 1/(number of choices)\n",
    "1.0/math.comb(9,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.190"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 9 - as a percentage\n",
    "1.0/math.comb(9,3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thus the probability of selecting 3 correct from the 9 cups is still >1%.  We need to add more 'random' cups.  Lets look at using a set of 10 cups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of ways of selecting 3 cups from 10 (number of choices)\n",
    "math.comb(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 9 is 1/(number of choices)\n",
    "1.0/math.comb(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.833"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probability of selecting 3 cups from 9 - as a percentage\n",
    "1.0/math.comb(10,3)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus 10 cups would be required if we were to let the taster get one cup wrong while maintaining the 1% threshold? \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise 2</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "Use <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html\" style=\"color: #ff791e\">scipy's version of Fisher's exact test</a> to simulate the Lady Tasting Tea problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Fisher exact test on a 2x2 contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import fisher_exact as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the scipy stats page \n",
    "\n",
    "Understand the concept\n",
    "\n",
    "Apply to fisher tea test\n",
    "\n",
    "Note: Fisher tea test was 4-4  wheresas scipy used 2-2\n",
    "\n",
    "Examine the code to see where 2-2 is used: can this be extended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can click on the [source] button and remove some of mystique <br>\n",
    "it hyperlinks to github <br>\n",
    "scipy is the name of the repository <br>\n",
    "asks to go to line 4492.  The scipystats_fisher function (code) is here <br>\n",
    "This is what is used i.e. code pulled out of comments from a repository in github <br> scipy_stats text is pulled from here <br>from line 4661 actually have the python code that executes the function <br>  e.g. 'return oddsratio, pvalue' line 4723 is the output of the python code <br>\n",
    "ultimately its just open source python code <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Statistical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistical using fisher exact get to similar output as itertools methods\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function scipy.stats._stats_py.fisher_exact(table, alternative='two-sided')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use this figure it out\n",
    "# in scipy stats if scroll down you will get loads of examples of how to do things\n",
    "ss.fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using fisher exact get to similar output as itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignificanceResult(statistic=inf, pvalue=0.028571428571428567)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "ss.fisher_exact([[4,0], [0,4]])\n",
    "# should have been 0.014285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.029"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turns out its double the chance\n",
    "# use scipy stats.fisher_exact to get the right result\n",
    "# but its giving the probability * 2\n",
    "# whats going on\n",
    "# go down to the code\n",
    "# use it to get the exepcted value\n",
    "# go back and understand why the statisitical code is giving the correct value- make sense of it \n",
    "# see if anything you can change in use of function without understanding the fisher-exact test\n",
    "# get comfortable with scipy.stats\n",
    "# reverse engineer scipy.stats to get the value you want\n",
    "# treat like learning how to drive the car\n",
    "# google lady tasting tea -scipy stats\n",
    "\n",
    "0.014285*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fisher_exact(table, alternative='two-sided'):\n",
    "    \"\"\"Perform a Fisher exact test on a 2x2 contingency table.\n",
    "    The null hypothesis is that the true odds ratio of the populations\n",
    "    underlying the observations is one, and the observations were sampled\n",
    "    from these populations under a condition: the marginals of the\n",
    "    resulting table must equal those of the observed table. The statistic\n",
    "    returned is the unconditional maximum likelihood estimate of the odds\n",
    "    ratio, and the p-value is the probability under the null hypothesis of\n",
    "    obtaining a table at least as extreme as the one that was actually\n",
    "    observed. There are other possible choices of statistic and two-sided\n",
    "    p-value definition associated with Fisher's exact test; please see the\n",
    "    Notes for more information.\n",
    "    Parameters\n",
    "    ----------\n",
    "    table : array_like of ints\n",
    "        A 2x2 contingency table.  Elements must be non-negative integers.\n",
    "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
    "        Defines the alternative hypothesis.\n",
    "        The following options are available (default is 'two-sided'):\n",
    "        * 'two-sided': the odds ratio of the underlying population is not one\n",
    "        * 'less': the odds ratio of the underlying population is less than one\n",
    "        * 'greater': the odds ratio of the underlying population is greater\n",
    "          than one\n",
    "        See the Notes for more details.\n",
    "    Returns\n",
    "    -------\n",
    "    oddsratio : float\n",
    "        This is prior odds ratio and not a posterior estimate.\n",
    "    p_value : float\n",
    "        P-value, the probability under the null hypothesis of obtaining a\n",
    "        table at least as extreme as the one that was actually observed.\n",
    "    See Also\n",
    "    --------\n",
    "    chi2_contingency : Chi-square test of independence of variables in a\n",
    "        contingency table.  This can be used as an alternative to\n",
    "        `fisher_exact` when the numbers in the table are large.\n",
    "    barnard_exact : Barnard's exact test, which is a more powerful alternative\n",
    "        than Fisher's exact test for 2x2 contingency tables.\n",
    "    boschloo_exact : Boschloo's exact test, which is a more powerful alternative\n",
    "        than Fisher's exact test for 2x2 contingency tables.\n",
    "    Notes\n",
    "    -----\n",
    "    *Null hypothesis and p-values*\n",
    "    The null hypothesis is that the true odds ratio of the populations\n",
    "    underlying the observations is one, and the observations were sampled at\n",
    "    random from these populations under a condition: the marginals of the\n",
    "    resulting table must equal those of the observed table. Equivalently,\n",
    "    the null hypothesis is that the input table is from the hypergeometric\n",
    "    distribution with parameters (as used in `hypergeom`)\n",
    "    ``M = a + b + c + d``, ``n = a + b`` and ``N = a + c``, where the\n",
    "    input table is ``[[a, b], [c, d]]``.  This distribution has support\n",
    "    ``max(0, N + n - M) <= x <= min(N, n)``, or, in terms of the values\n",
    "    in the input table, ``min(0, a - d) <= x <= a + min(b, c)``.  ``x``\n",
    "    can be interpreted as the upper-left element of a 2x2 table, so the\n",
    "    tables in the distribution have form::\n",
    "        [  x           n - x     ]\n",
    "        [N - x    M - (n + N) + x]\n",
    "    For example, if::\n",
    "        table = [6  2]\n",
    "                [1  4]\n",
    "    then the support is ``2 <= x <= 7``, and the tables in the distribution\n",
    "    are::\n",
    "        [2 6]   [3 5]   [4 4]   [5 3]   [6 2]  [7 1]\n",
    "        [5 0]   [4 1]   [3 2]   [2 3]   [1 4]  [0 5]\n",
    "    The probability of each table is given by the hypergeometric distribution\n",
    "    ``hypergeom.pmf(x, M, n, N)``.  For this example, these are (rounded to\n",
    "    three significant digits)::\n",
    "        x       2      3      4      5       6        7\n",
    "        p  0.0163  0.163  0.408  0.326  0.0816  0.00466\n",
    "    These can be computed with::\n",
    "        >>> from scipy.stats import hypergeom\n",
    "        >>> table = np.array([[6, 2], [1, 4]])\n",
    "        >>> M = table.sum()\n",
    "        >>> n = table[0].sum()\n",
    "        >>> N = table[:, 0].sum()\n",
    "        >>> start, end = hypergeom.support(M, n, N)\n",
    "        >>> hypergeom.pmf(np.arange(start, end+1), M, n, N)\n",
    "        array([0.01631702, 0.16317016, 0.40792541, 0.32634033, 0.08158508,\n",
    "               0.004662  ])\n",
    "    The two-sided p-value is the probability that, under the null hypothesis,\n",
    "    a random table would have a probability equal to or less than the\n",
    "    probability of the input table.  For our example, the probability of\n",
    "    the input table (where ``x = 6``) is 0.0816.  The x values where the\n",
    "    probability does not exceed this are 2, 6 and 7, so the two-sided p-value\n",
    "    is ``0.0163 + 0.0816 + 0.00466 ~= 0.10256``::\n",
    "        >>> from scipy.stats import fisher_exact\n",
    "        >>> oddsr, p = fisher_exact(table, alternative='two-sided')\n",
    "        >>> p\n",
    "        0.10256410256410257\n",
    "    The one-sided p-value for ``alternative='greater'`` is the probability\n",
    "    that a random table has ``x >= a``, which in our example is ``x >= 6``,\n",
    "    or ``0.0816 + 0.00466 ~= 0.08626``::\n",
    "        >>> oddsr, p = fisher_exact(table, alternative='greater')\n",
    "        >>> p\n",
    "        0.08624708624708627\n",
    "    This is equivalent to computing the survival function of the\n",
    "    distribution at ``x = 5`` (one less than ``x`` from the input table,\n",
    "    because we want to include the probability of ``x = 6`` in the sum)::\n",
    "        >>> hypergeom.sf(5, M, n, N)\n",
    "        0.08624708624708627\n",
    "    For ``alternative='less'``, the one-sided p-value is the probability\n",
    "    that a random table has ``x <= a``, (i.e. ``x <= 6`` in our example),\n",
    "    or ``0.0163 + 0.163 + 0.408 + 0.326 + 0.0816 ~= 0.9949``::\n",
    "        >>> oddsr, p = fisher_exact(table, alternative='less')\n",
    "        >>> p\n",
    "        0.9953379953379957\n",
    "    This is equivalent to computing the cumulative distribution function\n",
    "    of the distribution at ``x = 6``:\n",
    "        >>> hypergeom.cdf(6, M, n, N)\n",
    "        0.9953379953379957\n",
    "    *Odds ratio*\n",
    "    The calculated odds ratio is different from the one R uses. This SciPy\n",
    "    implementation returns the (more common) \"unconditional Maximum\n",
    "    Likelihood Estimate\", while R uses the \"conditional Maximum Likelihood\n",
    "    Estimate\".\n",
    "    Examples\n",
    "    --------\n",
    "    Say we spend a few days counting whales and sharks in the Atlantic and\n",
    "    Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the\n",
    "    Indian ocean 2 whales and 5 sharks. Then our contingency table is::\n",
    "                Atlantic  Indian\n",
    "        whales     8        2\n",
    "        sharks     1        5\n",
    "    We use this table to find the p-value:\n",
    "    >>> from scipy.stats import fisher_exact\n",
    "    >>> oddsratio, pvalue = fisher_exact([[8, 2], [1, 5]])\n",
    "    >>> pvalue\n",
    "    0.0349...\n",
    "    The probability that we would observe this or an even more imbalanced ratio\n",
    "    by chance is about 3.5%.  A commonly used significance level is 5%--if we\n",
    "    adopt that, we can therefore conclude that our observed imbalance is\n",
    "    statistically significant; whales prefer the Atlantic while sharks prefer\n",
    "    the Indian ocean.\n",
    "    \"\"\"\n",
    "    hypergeom = distributions.hypergeom\n",
    "    # int32 is not enough for the algorithm\n",
    "    c = np.asarray(table, dtype=np.int64)\n",
    "    if not c.shape == (2, 2):\n",
    "        raise ValueError(\"The input `table` must be of shape (2, 2).\")\n",
    "\n",
    "    if np.any(c < 0):\n",
    "        raise ValueError(\"All values in `table` must be nonnegative.\")\n",
    "\n",
    "    if 0 in c.sum(axis=0) or 0 in c.sum(axis=1):\n",
    "        # If both values in a row or column are zero, the p-value is 1 and\n",
    "        # the odds ratio is NaN.\n",
    "        return np.nan, 1.0\n",
    "\n",
    "    if c[1, 0] > 0 and c[0, 1] > 0:\n",
    "        oddsratio = c[0, 0] * c[1, 1] / (c[1, 0] * c[0, 1])\n",
    "    else:\n",
    "        oddsratio = np.inf\n",
    "\n",
    "    n1 = c[0, 0] + c[0, 1]\n",
    "    n2 = c[1, 0] + c[1, 1]\n",
    "    n = c[0, 0] + c[1, 0]\n",
    "\n",
    "    def pmf(x):\n",
    "        return hypergeom.pmf(x, n1 + n2, n1, n)\n",
    "\n",
    "    if alternative == 'less':\n",
    "        pvalue = hypergeom.cdf(c[0, 0], n1 + n2, n1, n)\n",
    "    elif alternative == 'greater':\n",
    "        # Same formula as the 'less' case, but with the second column.\n",
    "        pvalue = hypergeom.cdf(c[0, 1], n1 + n2, n1, c[0, 1] + c[1, 1])\n",
    "    elif alternative == 'two-sided':\n",
    "        mode = int((n + 1) * (n1 + 1) / (n1 + n2 + 2))\n",
    "        pexact = hypergeom.pmf(c[0, 0], n1 + n2, n1, n)\n",
    "        pmode = hypergeom.pmf(mode, n1 + n2, n1, n)\n",
    "\n",
    "        epsilon = 1e-14\n",
    "        gamma = 1 + epsilon\n",
    "\n",
    "        if np.abs(pexact - pmode) / np.maximum(pexact, pmode) <= epsilon:\n",
    "            return oddsratio, 1.\n",
    "\n",
    "        elif c[0, 0] < mode:\n",
    "            plower = hypergeom.cdf(c[0, 0], n1 + n2, n1, n)\n",
    "            if hypergeom.pmf(n, n1 + n2, n1, n) > pexact * gamma:\n",
    "                return oddsratio, plower\n",
    "\n",
    "            guess = _binary_search(lambda x: -pmf(x), -pexact * gamma, mode, n)\n",
    "            pvalue = plower + hypergeom.sf(guess, n1 + n2, n1, n)\n",
    "        else:\n",
    "            pupper = hypergeom.sf(c[0, 0] - 1, n1 + n2, n1, n)\n",
    "            if hypergeom.pmf(0, n1 + n2, n1, n) > pexact * gamma:\n",
    "                return oddsratio, pupper\n",
    "\n",
    "            guess = _binary_search(pmf, pexact * gamma, 0, mode)\n",
    "            pvalue = pupper + hypergeom.cdf(guess, n1 + n2, n1, n)\n",
    "    else:\n",
    "        msg = \"`alternative` should be one of {'two-sided', 'less', 'greater'}\"\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    pvalue = min(pvalue, 1.0)\n",
    "\n",
    "    return oddsratio, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'distributions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e1ae93d48dba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# example\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfisher_exact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# should have been 0.014285\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-2039a9766393>\u001b[0m in \u001b[0;36mfisher_exact\u001b[1;34m(table, alternative)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[0mthe\u001b[0m \u001b[0mIndian\u001b[0m \u001b[0mocean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \"\"\"\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mhypergeom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypergeom\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[1;31m# int32 is not enough for the algorithm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'distributions' is not defined"
     ]
    }
   ],
   "source": [
    "# example\n",
    "fisher_exact([[4,0], [0,4]])\n",
    "# should have been 0.014285"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise 3</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "Take the code from the <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\" style=\"color: #ff791e\">Examples section of the scipy stats documentation for independent samples t-tests</a>, add it to your own notebook and add explain how it works using MarkDown cells and code comments. Improve it in any way you think it could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test assumes that the populations have identical variances by default i.e. 'equal_var=True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If True (default), perform a standard independent 2 sample test that assumes equal population variances. <br>\n",
    "\n",
    "If False, perform Welch’s t-test, which does not assume equal population variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# rng = random number generator\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with sample with identical means:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs1 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
    "rvs2 = stats.norm.rvs(loc=5, scale=10, size=500, random_state=rng)\n",
    "stats.ttest_ind(rvs1, rvs2)\n",
    "# Output varies when re-run- rng?\n",
    "# which above parameters will reduce the output variability when run and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added parameter (equal_var=False) to function\n",
    "# Only slight difference in output from above??\n",
    "stats.ttest_ind(rvs1, rvs2, equal_var=False)\n",
    "# Output linked to cell output above- 'rng' above generates the n random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ttest_ind underestimates p for unequal variances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new set of values\n",
    "rvs3 = stats.norm.rvs(loc=5, scale=20, size=500, random_state=rng)\n",
    "stats.ttest_ind(rvs1, rvs3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Added parameter (equal_var=False) to function\n",
    "# Only slight difference in output from above??\n",
    "stats.ttest_ind(rvs1, rvs3, equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When n1 != n2, the equal variance t-statistic is no longer equal to the unequal variance t-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rvs5 = stats.norm.rvs(loc=8, scale=20, size=100, random_state=rng)\n",
    "stats.ttest_ind(rvs1, rvs5)\n",
    "# Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0046418707568707885)\n",
    "stats.ttest_ind(rvs1, rvs5, equal_var=False)\n",
    "# Ttest_indResult(statistic=-1.8686598649188084, pvalue=0.06434714193919686)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When performing a permutation test, more permutations typically yields more accurate results. Use a np.random.Generator to ensure reproducibility:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.ttest_ind(rvs1, rvs5, permutations=10000,random_state=rng)\n",
    "# Ttest_indResult(statistic=-2.8415950600298774, pvalue=0.0052994700529947)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take these two samples, one of which has an extreme tail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (56, 128.6, 12, 123.8, 64.34, 78, 763.3)\n",
    "b = (1.1, 2.9, 4.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trim keyword to perform a trimmed (Yuen) t-test. For example, using 20% trimming, trim=.2, the test will reduce the impact of one (np.floor(trim*len(a))) element from each tail of sample a. It will have no effect on sample b because np.floor(trim*len(b)) is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats.ttest_ind(a, b, trim=.2)\n",
    "# Ttest_indResult(statistic=3.4463884028073513,pvalue=0.01369338726499547)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why trim?: Trimmed means are robust estimators of central tendency. To compute a trimmed mean, we remove a predetermined amount of observations on each side of a distribution, and average the remaining observations. If you think you’re not familiar with trimmed means, you already know one famous member of this family: the median. Indeed, the median is an extreme trimmed mean, in which all observations are removed except one or two.\n",
    "\n",
    "Using trimmed means confers two advantages:\n",
    "\n",
    "Trimmed means provide a better estimation of the location of the bulk of the observations than the mean when sampling from asymmetric distributions;\n",
    "the standard error of the trimmed mean is less affected by outliers and asymmetry than the mean, so that tests using trimmed means can have more power than tests using the mean.\n",
    "Important point: if we use a trimmed mean in an inferential test (see below), we make inferences about the population trimmed mean, not the population mean. The same is true for the median or any other measure of central tendency. So each robust estimator is a tool to answer a specific question, and this is why different estimators can return different answers.\n",
    "\n",
    "To trim a datset it is first sorted and then a predetermined percentage at either end of the dataset are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### References\n",
    "<br>\n",
    "Ronald Fisher, a Bad Cup of Tea, and the Birth of Modern Statistics:  <br>\n",
    "https://www.sciencehistory.org/distillations/ronald-fisher-a-bad-cup-of-tea-and-the-birth-of-modern-statistics <br><br>\n",
    "Basic Statistics- Trimmed Means <br>https://garstats.wordpress.com/2017/11/28/trimmed-means/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: rgb(0, 91, 94);\">Statistics</h1>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Lady Tasting Tea</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/a/aa/Youngronaldfisher2.JPG\" style=\"border: 1px solid #ff791e\" width=\"200px\">\n",
    "\n",
    "The Lady Tasting Tea problem was first described by <a href=\"http://www-history.mcs.st-andrews.ac.uk/Biographies/Fisher.html\" style=\"color: #ff791e\">Ronald A. Fisher</a> in his book The Design of Experiments.\n",
    "\n",
    "He described the problem as follows.\n",
    "\n",
    "> A lady declares that by tasting a cup of tea made with milk she can discriminate whether the milk or the tea infusion was first added to the cup: We will consider the problem of designing an experiment by means of which this assertion can be tested. For this purpose let us first lay down a simple form of experiment with a view to studying its limitations and its characteristics, both those which appear to be essential to the experimental method, when well developed, and those which are not essential but auxiliary.\n",
    "\n",
    "> Our experiment consists in mixing eight cups of tea, four in one way and four in the other, and presenting them to the subject for judgment in a random order. The subject has been told in advance of what the test will consist, namely that she will be asked to taste eight cups, that these shall be four of each kind, and that they shall be presented to her in a random order, that is in an order not determined arbitrarily by human choice, but by the actual manipulation of the physical apparatus used in games of chance, cards, dice, roulettes, etc., or, more expeditiously, from a published collection of random sampling numbers purporting to give the actual results of such manipulation. Her task is to divide the 8 cups into two sets of 4, agreeing, if possible, with the treatments received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Number of Combinations</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Let us investigate the number of ways in which the lady can arrange the eight cups into two groups of four.\n",
    "\n",
    "The first group is the cups with milk first.\n",
    "\n",
    "The second group is the cups with tea first.\n",
    "\n",
    "Once we select four cups for the first group, the second group is made up of the remaining cups.\n",
    "\n",
    "So, we only really need to count how many different ways there are to select four cups from eight.\n",
    "\n",
    "<br>\n",
    "\n",
    "To begin we give each of the eight cups a label from 0 to 7.\n",
    "\n",
    "Note that the cups have no order in the experiment, we are labelling them with integers simply for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eight cups.\n",
    "cups = list(range(8))\n",
    "cups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the subject is trying to pick the four cups with milk in first.\n",
    "\n",
    "If the subject is picking the cups randomly then they are randomly picking one of seventy different options.\n",
    "\n",
    "By randomly, we mean that they can not really tell which cups have the milk in first.\n",
    "\n",
    "Therefore they have only a probability of 1/70 of getting the right answer.\n",
    "\n",
    "The number of ways of selecting four unordered items from eight is given by the [Choose](http://mathworld.wolfram.com/Choose.html) function in mathematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of ways of selecting four things from eight.\n",
    "# where the order doesnt matter\n",
    "(8*7*6*5)/(4*3*2*1)\n",
    "\n",
    "# 8 cups: 0 1 2 3 4 5 6 7\n",
    "# 4 spots:  _ _ _ _\n",
    "# select the 1st cup with tea in them. 1st most obvious is 2, then 0, then 7 then 5\n",
    "# 4 spots: 2 0 7 5\n",
    "#: 1st spot have 8 selection possibilities\n",
    "#: 2nd spot have 7 selection possibilities\n",
    "# 3rd spot: have 6 selection possibilities\n",
    "# 4th spot: have 5 selection possibilities\n",
    "# (8*7*6*5) possibilities.\n",
    "# Multiply is for every one of possibility for 1st spot are 7 choices of 2nd spot thus *7\n",
    "# (4*3*2*1)  i.e.  2-0-7-5 could also be 7-0-2-5 or 0-7-2-5 \n",
    "# order assumed in 8*7*6*5. Therefore have to take in to account how many times have we double counted the same combination\n",
    "# Order within group is not important.  Just that the cup is in the right group.\n",
    "# i.e. 2-0-7-5 same as 0-2-7-5 (these are in the 4 cups of Group 1 (tea first)). \n",
    "# now need to  remove the potential duplicates\n",
    "# /(4*3*2*1) does this\n",
    "# result is 70 possibilities to select the cups at random\n",
    "# one of these will get the 4 right cups in Group 1\n",
    "# 1/70 chance or 1.4% she will get it right by chance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These possibilities can be enumerated using the combinations function from the itertools Python package.\n",
    "\n",
    "It takes a list $l$ of items and a number $n$. It returns/generates each way of selecting $n$ unordered items from $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "# package in python that returns/generates each way of selecting n unordereditems from a list of elements/numbers n\n",
    "# note itertools starts below with 0,1,2,3 but this is simply the computer way of displaying\n",
    "# if this is remoevd no reason to start with 0,1,2,3\n",
    "\n",
    "poss = list(itertools.combinations(cups, 4))\n",
    "poss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Hypotheses</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "In the book, Fisher describes the *null hypothesis*.\n",
    "\n",
    "It is the statement to be countered by the experiment.\n",
    "\n",
    "In this case it is the hypothesis that the subject cannot tell the difference between a cup of tea that had milk in it first and one that had the tea in it first.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Null Hypothesis:** the subject can not tell if the cup had milk in it first or last.\n",
    "\n",
    "<br>\n",
    "\n",
    "Once we collect the experimental data we evaluate how likely we were to see such data if the null hypothesis is true.\n",
    "\n",
    "If it is very unlikely, then we may reject the null hypothesis.\n",
    "\n",
    "Typically we have an alternative hypothesis that we suggest rejecting the null hypothesis is evidence for.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Alternative Hypothesis:** the subject can tell.\n",
    "\n",
    "<br>\n",
    "\n",
    "Should the subject pick the correct four cups with milk in them first, there is only a 1 in 70 (~1.4%) chance of them having done that if they were simply guessing.\n",
    "\n",
    "Fisher considered that unlikely enough to reject the null hypothesis if they manage it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only one of the 70 randomly selected possibilities is the desired result.\n",
    "1 / 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "The above gives about a 1.5% chance of randomly selecting the correct cups. Calculate the minimum number of cups of tea required to ensure the probability of randomly selecting the correct cups is less than or equal to 1%.\n",
    "\n",
    "*Bonus:* How many would be required if you were to let the taster get one cup wrong while maintaining the 1% threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Distribution</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "We may be tempted to allow the possibility of rejecting the null hypothesis if the subject gets three cups right.\n",
    "\n",
    "That would be a mistake, as the following plot shows.\n",
    "\n",
    "It shows the null distribution, the number of ways of guessing for each number of correct cups.\n",
    "\n",
    "We can see from the plot that there is ~20% chance of guessing at least three correct cups.\n",
    "\n",
    "I don't think that's unlikely enough to reject the null hypothesis i.e. * that its still possible by a pure guess that 3 cups were correctly chosen*  \n",
    "\n",
    "Its much less possible by a pure guess that 4 cups were correctly chosen (1.4% in fact)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import seaborn as sns\n",
    "\n",
    "# Pick a random correct answer from the list of 70 to simulate the experiment. To be the 'milkfirst' cups. could have used 1st 4 but just showing random selection.\n",
    "milkfirst = set(random.choice(poss))\n",
    "\n",
    "# itertools combinations creates the random sets of 4\n",
    "# set doesnt have order, doesnt keep count, can't have repeats\n",
    "\n",
    "# Count the overlap (intersection) between the correct answer, and each of the 70 possiblities.\n",
    "counts = [len(milkfirst & set(i)) for i in itertools.combinations(cups, 4)]\n",
    "\n",
    "# give me the size of the overlaps either theres 0 overlap, 1 overlap, 2 overlaps, 3 overlaps, 4 overlaps etc\n",
    "# Create the plot.(Null distribution)\n",
    "sns.countplot(x=counts);\n",
    "\n",
    "# 1 opportunity of 0 overlaps. milkfirst selection has none right.  Exact opposite\n",
    "# 1 opportunity to get 4 right\n",
    "# 37 overlap 2 (2 right)\n",
    "# 15 overlap (1 right or 3 right)\n",
    "# thus if  allow one cup choice wrong then 20% chance (15/70*100) that they randomly select three correct ones\n",
    "# based on chance alone\n",
    "# null hypothesis is true is usually out at the tail (i.e. 5% CI) i.e. theres only 5% or less chance that they could\n",
    "# have chosen the correct cups randomly\n",
    "# In this case the null hypothesis (here defined as taster does not know which cups and choice is purely random)\n",
    "# can be rejected and taster can select the correct cups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "Use <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fisher_exact.html\" style=\"color: #ff791e\">scipy's version of Fisher's exact test</a> to simulate the Lady Tasting Tea problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new section week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">$t$-Tests</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "\n",
    "$t$-tests are among the most common statistical tests performed in world.\n",
    "\n",
    "\n",
    "This notebook focuses on the practicalities of performing $t$-tests in Python.\n",
    "\n",
    "For information about the $t$-test itself, I recommend reading [Laerd Statistics's Independent t-test using SPSS Statistics](https://statistics.laerd.com/spss-tutorials/independent-t-test-using-spss-statistics.php).\n",
    "$t$-tests are among the most common statistical tests performed in world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>The t test estimates the true difference between two group means using the ratio of the difference in group means over the pooled standard error of both groups.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text for screen readers](http://localhost:8888/files/images/t-test%20formula.png?_xsrf=2%7Cddfa8db4%7C54cd136eb57f3fe394d47cabdf8b0e3b%7C1670523383 \"T-test formula\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A larger t value shows that the difference between group means is greater than the pooled standard error, indicating a more significant difference between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the groups come from a single population (e.g., measuring before and after an experimental treatment), perform a **paired t test**. This is a within-subjects design.  \n",
    "\n",
    "If the groups come from two different populations (e.g., two different species, or people from two separate cities), **perform a two-sample t test (a.k.a. independent t test)**. This is a between-subjects design.  \n",
    "\n",
    "If there is one group being compared against a standard value (e.g., comparing the acidity of a liquid to a neutral pH of 7), **perform a one-sample t test.**  \n",
    "\n",
    "If you only care whether the two populations are different from one another, **perform a two-tailed t test.**  \n",
    "\n",
    "If you want to know whether one population mean is greater than or less than the other, **perform a one-tailed t test.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Packages</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "One of Python's strengths is the quality of numerical packages available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient numerical arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Data frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Main statistics package.\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Alternative statistics package.\n",
    "import statsmodels.stats.weightstats as stat\n",
    "\n",
    "# Plotting.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fancier plotting.\n",
    "import seaborn as sns\n",
    "\n",
    "# Better sized plots.\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Nicer colours and styles for plots.\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Simulated Data</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "We can create fake data sets with specific properties to investigate numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### can play with mean and stddev of both data sets below and re-plot.\n",
    "#### closer mean and greater stddev makes it more difficult to ascertain same or different populations\n",
    "#### solution is to run the t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for two different lists of numbers.\n",
    "m_a, s_a, m_b, s_b = 1.0, 0.4, 2.0, 0.4\n",
    "# Sample size.\n",
    "N = 40\n",
    "\n",
    "# Create two lists of numbers based on bell-shaped probability curves.\n",
    "a = np.random.normal(loc=m_a, scale=s_a, size=N)\n",
    "b = np.random.normal(loc=m_b, scale=s_b, size=N)\n",
    "\n",
    "# Stick both samples in one data frame.\n",
    "df = pd.DataFrame({'Category': ['A'] * len(a) + ['B'] * len(b), 'Value': np.hstack([a,b])})\n",
    "\n",
    "# We can look directly at the list of numbers, but it's not very illuminating.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Visualisation</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "A good plot can quickly show us what the numbers look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One type of plot available in seaborn.\n",
    "sns.catplot(x='Category', y='Value', jitter=False, data=df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">The $t$-Test</h2>\n",
    "\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Running a t-test in Python is done with a single function call. You can use scipy or statsmodels, amongst others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scipy.stats version.\n",
    "t_ss, p_ss = ss.ttest_ind(a, b)\n",
    "print(f\"t-value: {t_ss}\\tp-value: {p_ss}\")\n",
    "print(f\"P_scipy: {p_ss:0.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The statsmodels version.\n",
    "t_sm, p_sm, d_sm = stat.ttest_ind(a, b)\n",
    "print(f\"t-value: {t_sm}\\tp-value: {p_sm}\\tDeg Free: {d_sm}\")\n",
    "print(f\"P_statsmodels: {p_sm:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate t statistic \"by hand\".\n",
    "# https://en.wikipedia.org/wiki/Test_statistic\n",
    "\n",
    "#Two-sample pooled t-test, equal variances\n",
    "\n",
    "# Length of the arrays.\n",
    "n1 = len(a)\n",
    "n2 = len(b)\n",
    "\n",
    "# Means of the samples.\n",
    "m1 = np.sum(a) / n1\n",
    "m2 = np.sum(b) / n2\n",
    "\n",
    "# Sample standard deviations.\n",
    "s1 = np.sqrt(np.sum((a - m1)**2) / (n1 - 1))\n",
    "s2 = np.sqrt(np.sum((b - m2)**2) / (n1 - 1))\n",
    "\n",
    "df = n1 + n2 - 2\n",
    "sp2 = ((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / df\n",
    "t = (m1 - m2) / (np.sqrt(sp2) * np.sqrt(1.0/n1 + 1.0/n2))\n",
    "\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Populations</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "$t$-tests perform calculations on samples from two populations to test whether the populations are likely similar.\n",
    "\n",
    "In the real world, we only see the samples and we cannot see the populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a plot with the following x values.\n",
    "# can reuse the same mean & stdev values for a & b above\n",
    "# m_a = mean 'a', m_b = mean 'b', \n",
    "# s_a = stdev 'a', s_b = stdev 'b', \n",
    "# pull x values from -5 to +5 stdev to ensure bell curve pop is reasonably covered\n",
    "min_x = min(m_a, m_b) - 5.0 * max(s_a, s_b)\n",
    "max_x = max(m_a, m_b) + 5.0 * max(s_a, s_b)\n",
    "\n",
    "# create the values of x within the range\n",
    "x = np.linspace(min_x, max_x, 1000)\n",
    "\n",
    "# We'll have plots of two different populations on one set of axes.\n",
    "# These are normal probability density functions (pdf).\n",
    "# See: https://en.wikipedia.org/wiki/Normal_distribution\n",
    "y_a = ss.norm.pdf(x, m_a, s_a)\n",
    "y_b = ss.norm.pdf(x, m_b, s_b)\n",
    "\n",
    "# Create and show the plot.\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax.plot(x, y_a)\n",
    "ax.plot(x, y_b)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Critical Value</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "The critical value is used to make a decision regarding the calculation of the $t$ statistic from the samples.\n",
    "\n",
    "If the probability, of seeing such a $t$ value given the hypothesis that there is no difference between the means, is low, then data is suggesting that you should reject that hypothesis.  \n",
    "\n",
    "In this case the means come from different populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critical probability value.\n",
    "critical = 0.05\n",
    "\n",
    "# Create the figure.\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# A range of x-values - these represent the t statistic.\n",
    "min_x = -5.0\n",
    "max_x = 5.0\n",
    "x = np.linspace(min_x, max_x, 1000)\n",
    "\n",
    "# The probability density function of the t statistic.\n",
    "# Here we use the degrees of freedom from above.\n",
    "t = ss.t.pdf(x, d_sm)\n",
    "# Plot it.\n",
    "ax.plot(x, t, color='red')\n",
    "\n",
    "# Get the tails.\n",
    "tf = pd.DataFrame({'x': x, 't': t})\n",
    "tcrit = abs(ss.t.ppf(critical / 2.0, d_sm))\n",
    "tail_one = tf[tf['x'] >= tcrit]\n",
    "tail_two = tf[tf['x'] <= -tcrit]\n",
    "# Plot them.\n",
    "ax.fill_between(tail_one['x'], tail_one['t'], 0, facecolor=\"red\")\n",
    "ax.fill_between(tail_two['x'], tail_two['t'], 0, facecolor=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Type I errors - False Positives</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run 10000 t-tests where the population means are equal.\n",
    "# We should make the wrong decision (reject the hypothesis) (100 * critical) percent of the time.\n",
    "\n",
    "# The number of trials to run.\n",
    "trials = 10000\n",
    "# The number of values in each sample.\n",
    "N = 100\n",
    "# Population 1 mean, population 2 mean, standard deviation in both.\n",
    "mean1, mean2, stddev = 2.0, 2.0, 0.3\n",
    "# Critical probability value.\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors commited.\n",
    "rejects = 0\n",
    "\n",
    "# Loop throguh trials.\n",
    "for i in range(trials):\n",
    "    # Generate sample 1.\n",
    "    sample1 = np.random.normal(loc=mean1, scale=stddev, size=N)\n",
    "    # Generate sample 2.\n",
    "    sample2 = np.random.normal(loc=mean2, scale=stddev, size=N)\n",
    "    # Run the t-test.\n",
    "    t, p = ss.ttest_ind(sample1, sample2)\n",
    "    # If p is less than critical, reject.\n",
    "    if p <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Print results.\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei:0.2f}%\")\n",
    "\n",
    "# Thus output is the frequency of REJECTING the null hypothesis (based on 0.05 threshold) given as a percentage\n",
    "# Thus desipte np.random instruced to use same mean to create sample1 & sample 2 datasets.\n",
    "# ~5% false positives-'significant difference in means' and null hypothesis erroneously rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Type II errors - False Negatives</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "The chance of a false negative is harder to quantify.\n",
    "\n",
    "It depends on how close the means are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run 10000 t-tests where the population means are NOT equal.\n",
    "# How often will we not reject the hypothesis?\n",
    "\n",
    "# The number of trials to run.\n",
    "trials = 10000\n",
    "# The number of values in each sample.\n",
    "N = 100\n",
    "# Population 1 mean, population 2 mean, standard deviation in both.\n",
    "mean1, mean2, stddev = 2.0, 2.1, 0.3\n",
    "# Critical probability value.\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors commited.\n",
    "notrejects = 0\n",
    "\n",
    "# Loop throguh trials.\n",
    "for i in range(trials):\n",
    "    # Generate sample 1.\n",
    "    sample1 = np.random.normal(loc=mean1, scale=stddev, size=N)\n",
    "    # Generate sample 2.\n",
    "    sample2 = np.random.normal(loc=mean2, scale=stddev, size=N)\n",
    "    # Run the t-test.\n",
    "    t, p = ss.ttest_ind(sample1, sample2)\n",
    "    # If p is greater than critical, do not reject.\n",
    "    if p > critical:\n",
    "        notrejects = notrejects + 1\n",
    "\n",
    "# Print results.\n",
    "typeii = 100.0 * (notrejects / trials)\n",
    "print(f\"{typeii:0.2f}%\")\n",
    "\n",
    "# Thus output is the frequency of accepting of the null hypothesis (based on 0.05 threshold) given as a percentage\n",
    "# Thus approx 1 in 3 t-tests accepts the null hypothesis (means from same population) \n",
    "# desipte np.random instruced to use DIFFERENT mean (2.0 & 2.1) to create sample1 & sample 2 datasets.\n",
    "# ~35% false positives-'no significant difference in means'. Null hypothesis erroneously accepted\n",
    "# Note: reduces to 0.3% if mean (2.0 & 2.2) used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Paired Samples</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Here we try a slightly different $t$ test - one based on repeated measures.\n",
    "\n",
    "*References for this section:*\n",
    "\n",
    "[Vincent Arel-Bundock's R datasets list](https://vincentarelbundock.github.io/Rdatasets/articles/data.html)\n",
    "\n",
    "[t-test: Comparing Group Means](https://uc-r.github.io/t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsleep = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/sleep.csv\")\n",
    "dfsleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first sample from the data set.\n",
    "drugA = dfsleep[dfsleep[\"group\"] == 1]\n",
    "drugA = drugA.sort_values(\"ID\")\n",
    "drugA = drugA[\"extra\"].to_numpy()\n",
    "drugA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the second sample from the data set.\n",
    "drugB = dfsleep[dfsleep[\"group\"] == 2]\n",
    "drugB = drugB.sort_values(\"ID\")\n",
    "drugB = drugB[\"extra\"].to_numpy()\n",
    "drugB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a paired samples t-test.\n",
    "ss.ttest_rel(drugA, drugB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In one way, this is equivalent to a one sample t-test.\n",
    "ss.ttest_1samp(drugB - drugA, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# That's how statsmodels suggests you do the test.\n",
    "stat.DescrStatsW(drugB - drugA).ttest_mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Problems with multiple $t$-tests</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "Suppose we want to compare three groups. The null hypothesis is that the population means are all equal. Can three $t$ tests be run in parallel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of each sample.\n",
    "N = 100\n",
    "\n",
    "# Create three samples.\n",
    "sampA = np.random.normal(1.0, 0.2, N)\n",
    "sampB = np.random.normal(1.0, 0.2, N)\n",
    "sampC = np.random.normal(2.0, 0.2, N)\n",
    "\n",
    "# Put samples in a single data frame.\n",
    "sample = ['A'] * N + ['B'] * N + ['C'] * N\n",
    "values = np.hstack([sampA, sampB, sampC])\n",
    "dfsamps = pd.DataFrame({'Sample': sample, 'Value': values})\n",
    "\n",
    "# Visualise samples.\n",
    "sns.catplot(x='Sample', y='Value', jitter=False, data=dfsamps);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-Tests - one for each pair.\n",
    "t_AB, p_AB = ss.ttest_ind(sampA, sampB)\n",
    "t_AC, p_AC = ss.ttest_ind(sampA, sampC)\n",
    "t_BC, p_BC = ss.ttest_ind(sampB, sampC)\n",
    "\n",
    "print(f\"p_AB: {p_AB:.2f}\\tp_AC: {p_AC:.2f}\\tp_BC: {p_BC:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run 10000 t-tests where the population means are equal.\n",
    "# We should make the wrong decision (reject the hypothesis) (100 * critical) percent of the time.\n",
    "# We expect to incorrectly reject the null hypothesis 5% of the time.\n",
    "\n",
    "# The number of trials to run.\n",
    "trials = 10000\n",
    "# The number of values in each sample.\n",
    "N = 100\n",
    "# Population 1 mean, population 2 mean, population 3 mean, standard deviation in both.\n",
    "mean1, mean2, mean3, stddev = 2.0, 2.0, 2.0, 0.3\n",
    "# Critical probability value.\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors commited.\n",
    "rejects = 0\n",
    "\n",
    "# Loop throguh trials.\n",
    "for i in range(trials):\n",
    "    # Generate sample 1.\n",
    "    sample1 = np.random.normal(loc=mean1, scale=stddev, size=N)\n",
    "    # Generate sample 2.\n",
    "    sample2 = np.random.normal(loc=mean2, scale=stddev, size=N)\n",
    "    # Generate sample 3.\n",
    "    sample3 = np.random.normal(loc=mean3, scale=stddev, size=N)\n",
    "    # Run the t-tests.\n",
    "    t1, p1 = ss.ttest_ind(sample1, sample2)\n",
    "    t2, p2 = ss.ttest_ind(sample1, sample3)\n",
    "    t3, p3 = ss.ttest_ind(sample2, sample3)\n",
    "    # If any is less than critical, reject.\n",
    "    if p1 <= critical or p2 <= critical or p3 <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Print results.\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei:0.2f}%\")\n",
    "\n",
    "# Thus output is the frequency of REJECTING the null hypothesis (based on 0.05 threshold) given as a percentage\n",
    "# Thus desipte np.random instruced to use same mean to create sample1 & sample 2 & sample 3 datasets.\n",
    "# ~12% of times false positives-'significant difference in means' and null hypothesis erroneously rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color: rgb(0, 91, 94);\">Analaysis of Variance (ANOVA)</h2>\n",
    "\n",
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "ANOVA can be used to avoid a higher Type I error rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html\n",
    "F, P = ss.f_oneway(sampA, sampB, sampC)\n",
    "print(f\"F:{F:.2f} P:{P:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run 10000 ANOVAs where the population means are equal.\n",
    "# We should make the wrong decision (reject the hypothesis) (100 * critical) percent of the time.\n",
    "# We expect to incorrectly reject the null hypothesis 5% of the time.\n",
    "\n",
    "# The number of trials to run.\n",
    "trials = 10000\n",
    "# The number of values in each sample.\n",
    "N = 100\n",
    "# Population 1 mean, population 2 mean, population 3 mean, standard deviation in both.\n",
    "mean1, mean2, mean3, stddev = 2.0, 2.0, 2.0, 0.3\n",
    "# Critical probability value.\n",
    "critical = 0.05\n",
    "\n",
    "# Running total of type I errors commited.\n",
    "rejects = 0\n",
    "\n",
    "# Loop throguh trials.\n",
    "for i in range(trials):\n",
    "    # Generate sample 1.\n",
    "    sample1 = np.random.normal(loc=mean1, scale=stddev, size=N)\n",
    "    # Generate sample 2.\n",
    "    sample2 = np.random.normal(loc=mean2, scale=stddev, size=N)\n",
    "    # Generate sample 3.\n",
    "    sample3 = np.random.normal(loc=mean3, scale=stddev, size=N)\n",
    "    # Run the test.\n",
    "    F, p = ss.f_oneway(sample1, sample2, sample3)\n",
    "    # If any is less than critical, reject.\n",
    "    if p <= critical:\n",
    "        rejects = rejects + 1\n",
    "\n",
    "# Print results.\n",
    "typei = 100.0 * (rejects / trials)\n",
    "print(f\"{typei:0.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color: #001a79;\">Exercise</h3>\n",
    "\n",
    "<hr style=\"border-top: 1px solid #001a79;\" />\n",
    "\n",
    "<i style=\"color: #001a79;\">Remember to do these exercises in your own notebook in your assessment repository.</i>\n",
    "\n",
    "Take the code from the <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html\" style=\"color: #ff791e\">Examples section of the scipy stats documentation for independent samples t-tests</a>, add it to your own notebook and add explain how it works using MarkDown cells and code comments. Improve it in any way you think it could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 1px solid rgb(0, 91, 94);\" />\n",
    "\n",
    "<h2 style=\"color: rgb(0, 91, 94);\">End</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

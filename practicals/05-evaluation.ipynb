{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Anomaly Detection\n",
    "\n",
    "***\n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_anomaly_detection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "\n",
    "what is tensorflow\n",
    "\n",
    "what is keras \n",
    "\n",
    "summary of what were tryign to do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Spreadsheet-like Data Frames.\n",
    "import pandas as pd\n",
    "\n",
    "# Layers of Neural networks.\n",
    "import tensorflow.keras as keras \n",
    "\n",
    "# then change code below where layers.input to keras.layers.input\n",
    "# no longer require from tensorflow.keras import layers\n",
    "# makes the code function clearer to 3rd party - in code cell know where the layers are coming from\n",
    "# anytime change code- rerun the kernel\n",
    "# get in the habit of restart/run all\n",
    "# keep record of changes\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "copied and pasted each code from the above link into the notebook\n",
    "\n",
    "can explore github in link/ look at raw source code\n",
    "\n",
    "jupyter notebook available- could download this simply but better to:\n",
    "\n",
    "- copy and paste in the libraries/code one by one and run it one by one\n",
    "\n",
    "- check you get the same output as webpage\n",
    " \n",
    "- look at what the code is doing/ run the plots \n",
    " \n",
    "- try to explain what is happening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data analytics: domain specific knowledge- someone might show this issue to you/someone else did this\n",
    "\n",
    "can you figure it out/ this shouldnt stop you exploring - figuring out\n",
    "\n",
    "you may start by knowing v little other than use numpy, ploy, keras, training the model, test data etc\n",
    "\n",
    "initially not a lot we understand, goal is to explain it in the notebook\n",
    "\n",
    "start by checking the code- do they run?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "***\n",
    "\n",
    "https://www.kaggle.com/datasets/boltzmannbrain/nab\n",
    "\n",
    "https://github.com/numenta/NAB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numenta Anomaly Benchmark (NAB) - look this up from kaggle above\n",
    "\n",
    " all data is fully open source\n",
    " \n",
    " tell you all about tyhe datasets in there\n",
    "\n",
    "kaggle has a link to github including code\n",
    "\n",
    "two different csv files loaded in code\n",
    " \n",
    " - artificial no anomaly/art_daily_small_noise,csv\n",
    " - artificial with anomaly/art_daily_small_noise,csv etetc\n",
    "seems to be 2 different datasets\n",
    "\n",
    "expect explanation of the two different links contents\n",
    "\n",
    "lot of markdowns explaining where data set comes from\n",
    "\n",
    "what it contains \n",
    "\n",
    "make plots llok nicer?\n",
    "\n",
    "lot more explanations of whats going on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'Refactoring the code'- modify it, clean it up etc \n",
    "#### Get a much greater feel of whatds going on\n",
    "#### check whats in url\n",
    "doesn't explain whats going on <br>\n",
    "write a narrative for classmate so they could understand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if  try to go to master_url_root below \n",
    "\n",
    "returns 404 in browser \n",
    "\n",
    "url is only meant to link to raw github content\n",
    "\n",
    "code is getting pandas to read csv file - 2 different csvs\n",
    "\n",
    "betetr to gp directly to github itself\n",
    "\n",
    "i.e. its in the numenta-NAB-master- data sub folder\n",
    "\n",
    "searck through these- find the CSV's\n",
    "\n",
    "if pd.read_csv(url)  it will error i.e. can't parse\n",
    "\n",
    "why: whats shown in github is html  (same as ipynb)\n",
    "\n",
    "-click on raw- get the raw file\n",
    "\n",
    "create a str x2\n",
    "\n",
    "no_anomaly_url = <url>\n",
    "with_anomaly_url = <url>\n",
    "make urls direct links to csv\n",
    "see below root url + link to csv\n",
    "\n",
    "in python do not have a code >79 chars (Python style guide  as per https://peps.python.org/pep-0008/)\n",
    "\n",
    "hence see below root url + link to csv to tidy up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root of URLs.\n",
    "root_url = 'https://raw.githubusercontent.com/numenta/NAB/master/data/'\n",
    "\n",
    "# Data file without anomaly.\n",
    "no_anomaly_url = root_url + 'artificialNoAnomaly/art_daily_small_noise.csv'\n",
    "\n",
    "# Data file with anomaly.\n",
    "with_anomaly_url = root_url + 'artificialWithAnomaly/art_daily_jumpsup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the no_anomaly_url\n",
    "no_anomaly_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the with_anomaly_url\n",
    "with_anomaly_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define df_small_noise as read in csv\n",
    "## describe the connection between no_anomaly_url and small_noise\n",
    "# i.e. why call it that\n",
    "df_small_noise = pd.read_csv(no_anomaly_url, parse_dates=True, index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets havea look at dataset\n",
    "df_small_noise.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "df_small_noise.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas show the 1st value in df\n",
    "df_small_noise.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas sreturn the numerical value in df  where'value'\n",
    "df_small_noise.iloc[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get the first numerical value in the df\n",
    "df_small_noise.iloc[0].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply the value x 10e12\n",
    "df_small_noise.iloc[0].values[0] * 10e12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whats the shape of the df\n",
    "df_small_noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the fig size\n",
    "# fig, ax = plt.subplots(figsize=(12,6))\n",
    "# used pandas (plt) to plot a pamdas dataframe\n",
    "# plotting onto ax i.e. axes\n",
    "# use pyplot to increase fig size as desired\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "# Use pandas to plot data frame.\n",
    "# semicolon prevents the printout of the return value\n",
    "df_small_noise.plot(legend=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define df_daily_jumpsup as read in csv\n",
    "# describe the connection between with_anomaly_url and daily_jumsup\n",
    "# i.e. why call it that\n",
    "df_daily_jumpsup = pd.read_csv(with_anomaly_url, parse_dates=True, index_col=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printout the first 5 values in df\n",
    "df_daily_jumpsup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the dataframe\n",
    "print(df_daily_jumpsup.describe())\n",
    "# print function in notebook doesnt look as nice\n",
    "# better to leave out the print function as below\n",
    "# pandas is clever in jupyter notebooks\n",
    "# df_daily_jumpsup.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas return the 1st value in df\n",
    "df_daily_jumpsup.iloc[0]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the shape of the df\n",
    "df_daily_jumpsup.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the fig size\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "# Use pandas to plot data frame.\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "###### values clean-up\n",
    "##### explanation of why preprocessing is required\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the mean of small noise\n",
    "train_mean = df_small_noise.mean()\n",
    "\n",
    "train_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the sd of small noise\n",
    "train_std = df_small_noise.std()\n",
    "\n",
    "train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the mean from the value/divide by stdev\n",
    "# essentially gets all values normalized between x and y centered on zero\n",
    "# standard trick in neural networks- subtract the mean from the value/divide by stdev\n",
    "# essentially gets all avlues normalized between x and y centered on zero\n",
    "# df_train_vals not df_train as thats already in use\n",
    "df_train_vals = (df_small_noise - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas return the 1st 5 values in the dataframe\n",
    "df_train_vals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas return the last 5 values in the dataframe\n",
    "df_train_vals.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first plot data frame to have a look\n",
    "# \n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "# Use pandas to plot data frame.\n",
    "df_train_vals.plot(legend=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create \n",
    "df_small_noise['less_mean'] = df_small_noise['value'] - df_small_noise['value'].mean()\n",
    "df_small_noise['div_std'] = df_small_noise['less_mean'] / df_small_noise['value'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_noise['value'].mean(), df_small_noise['less_mean'].mean(), df_small_noise['div_std'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small_noise['value'].std(), df_small_noise['less_mean'].std(), df_small_noise['div_std'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define size of plot\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "# Use pandas to plot data frame.\n",
    "df_train_vals.plot(legend=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the dataframe shape\n",
    "# so 4032 df_train vals\n",
    "df_train_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the dataframe shape\n",
    "# so 4032 df_train vals\n",
    "df_train_vals.columns\n",
    "# so col names are values , datatype = object\n",
    "# value is name of column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### break down the function as per ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the value of 'time_steps'\n",
    "# which the code originally used\n",
    "# renamed to window_size below\n",
    "time_steps =288"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated training sequences for use in the model\n",
    "#def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    #output = []\n",
    "    #for i in range(len(values)- time_steps +1):\n",
    "        #output.append(values[i : (i+ time_steps)])\n",
    "    #return np.stack(output)\n",
    "\n",
    "#def create_sequences(values, time_steps):\n",
    "    #output = []\n",
    "    #for i in range(len(values)- time_steps +1):\n",
    "        #output.append(values[i : (i+ time_steps)])\n",
    "    #return np.stack(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally was 'time_steps'\n",
    "# slide a window of size 288 across the linear dataset\n",
    "# grab each 288 into a new array/layer\n",
    "window_size = 288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### what they are doing to process the data ready for  reading into keras\n",
    "###### each of these are whats fed into the NN\n",
    "Window example:\n",
    "\n",
    "```python\n",
    "[1, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5]\n",
    "window_size = 3\n",
    "[1, 1, 2]\n",
    "[1, 2, 4]\n",
    "[2, 4, 5]\n",
    "[4, 5, 6]\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window list example.\n",
    "L = [1, 1, 2, 4, 5, 6, 7, 1, 2, 3, 4, 5]\n",
    "# Example window size.\n",
    "win_size = 3\n",
    "# Length of L.\n",
    "len(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of windows from L.\n",
    "len(L) - (win_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove brackets.\n",
    "len(L) - win_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the windows.\n",
    "for i in range(len(L) - win_size + 1):\n",
    "    print(L[i:i + win_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For accumulating the windows.\n",
    "wins = []\n",
    "\n",
    "# Generate the windows\n",
    "for i in range(len(L) - win_size + 1):\n",
    "    wins.append(L[i:i + win_size])\n",
    "\n",
    "wins\n",
    "# outpus the linear array above in datadrame of len 3 starting with each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D numpy array from wins.\n",
    "# output as individual arrys\n",
    "np.stack(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to create correct data shape for input to NN (Keras)\n",
    "def windows(vals, N=window_size):\n",
    "  # define an empty array for values\n",
    "  L = []\n",
    "  # iterate through vals for all windows of len N \n",
    "  for i in range(len(vals) - N + 1):\n",
    "    # append another row of N vals to L[]\n",
    "    L.append(vals[i:i+N])\n",
    "  # Stack i.e output L as individual arrays\n",
    "  return np.stack(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a numpy array\n",
    "# call it x_train and create it from the original dataset\n",
    "# transforn the original df (4032,1) to x_train((3745, 288, 1)) using def windows()\n",
    "x_train = windows(df_train_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the array first value [0] and last value [-1]\n",
    "x_train[0][0], x_train[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get 1st and last values from dataframe using iloc\n",
    "df_train_vals.iloc[0], df_train_vals.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way to get 100th value from dataframe using iloc\n",
    "df_train_vals.iloc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the array first value [0] and value [1]\n",
    "x_train[0][0], x_train[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review original shape of df -i.e 4032 values\n",
    "df_train_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check shape\n",
    "x_train.shape\n",
    "#windows of size 288 ,3745 of these X_train examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the values where 'value'\n",
    "df_train_vals['value'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 1st window of 288 values\n",
    "x_train[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "window_no = 200\n",
    "\n",
    "# \n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# \n",
    "y = df_train_vals['value'].values\n",
    "\n",
    "# \n",
    "ax.plot(np.arange(y.shape[0]), y, label='signal')\n",
    "\n",
    "# The first window.\n",
    "w = x_train[window_no].flatten()\n",
    "\n",
    "# \n",
    "ax.plot(np.arange(w.shape[0]) + window_no, w, label='window')\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get less windows than values(4032) as effectively run out of '288' \n",
    "# numbers remaining in array cannot make-up a 288 window\n",
    "# thus not used\n",
    "# keras needs the third dimension below i.e. '1'\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create layers for keras\n",
    "# check original code\n",
    "# model =keras.Sequential(\n",
    "#[\n",
    " # Layers = ######\n",
    "    \n",
    "# Neater syntax is to extract the 'Layers'\n",
    "# keep indent\n",
    "# put comments in each layer\n",
    "# Google: keras.layers.input \n",
    "# Google: keras.layers.Conv1D\n",
    "# research what the code means and describe the layer code meaning comments below\n",
    "\n",
    "layers = [\n",
    "  #\n",
    "  keras.layers.Input(shape=(x_train.shape[1], x_train.shape[2])),\n",
    "  #\n",
    "  keras.layers.Conv1D(\n",
    "    filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  #\n",
    "  keras.layers.Dropout(rate=0.2),\n",
    "  #\n",
    "  keras.layers.Conv1D(\n",
    "    filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  #\n",
    "  keras.layers.Conv1DTranspose(\n",
    "    filters=16, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  #\n",
    "  keras.layers.Dropout(rate=0.2),\n",
    "  #\n",
    "  keras.layers.Conv1DTranspose(\n",
    "    filters=32, kernel_size=7, padding=\"same\", strides=2, activation=\"relu\"),\n",
    "  #\n",
    "  keras.layers.Conv1DTranspose(filters=1, kernel_size=7, padding=\"same\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in layers var into keras Sequential function\n",
    "# model is an instance of keras.Sequential\n",
    "model = keras.Sequential(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimiser = Adam - find out what is this\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the output 'model'\n",
    "# look as mse - find out what is this\n",
    "model.compile(optimizer=optimizer, loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out a summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit function\n",
    "# explained here: https://keras.io/api/models/model_training_apis/\n",
    "\n",
    "# training data fed in with both input and output data\n",
    "# df_small_noise should be called 'df_no_anomaly'\n",
    "# training funvtion when run\n",
    "# original stopped after epoch 15\n",
    "\n",
    "# this code stops at epoch 12/50\n",
    "# Why?\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "##### evaluating the model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at NAB github repository\n",
    "# READ.me has a list of which model works well with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history is the fitted model\n",
    "# training loss [\"loss\"] and validation loss {\"val_loss\"]\n",
    "\n",
    "# keras. history- there is a history dict that returned from model.fit\n",
    "# look it up\n",
    "# output the values\n",
    "history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the values\n",
    "history.history[\"val_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas fig -define the size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# plot values of history.history[\"loss\"]\n",
    "ax.plot(history.history[\"loss\"], label=\"Training Loss\")\n",
    "# plot values of history.history[\"val_loss\"]\n",
    "ax.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\n",
    "# \n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original code from url:\n",
    "# x_train_pred = model.predict(x_train)\n",
    "# train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)\n",
    "# plotting code\n",
    "\n",
    "# clearer to sepearte it out as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train MAE loss.\n",
    "# twhat doing: get the mode that s trained to predict the vaslues out of the training set\n",
    "# looking for anomalies\n",
    "x_train_pred = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss...\n",
    "# standard way to see how accurate you are\n",
    "# take the predicted values- take absolute value.  Plot\n",
    "train_mae_loss = np.mean(np.abs(x_train_pred - x_train), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas fig size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "# create a histogram\n",
    "ax.hist(train_mae_loss, bins=50)\n",
    "# add x label\n",
    "ax.set_xlabel(\"Train MAE loss\")\n",
    "#add y label\n",
    "ax.set_ylabel(\"No of samples\");\n",
    "\n",
    "# no requirement for plt(show) in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction loss threshold.\n",
    "# declare threshold\n",
    "threshold = np.max(train_mae_loss)\n",
    "\n",
    "# print out the threshold\n",
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas fig size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# define x axis\n",
    "ax.plot(x_train[0])\n",
    "\n",
    "# define y axis\n",
    "ax.plot(x_train_pred[0]);\n",
    "\n",
    "# when training on the window things get smoothed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract the training mean and training stdev from the values\n",
    "\n",
    "# train_mean and train_std as defined above\n",
    "#  new dataset = df_daily_jumpsup\n",
    "tapply anything from training set to new dataset hence same operation\n",
    "# get final dataset we can plot 'df_test_value'\n",
    "df_test_value = (df_daily_jumpsup - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pandas fig size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "#  plot df_test_value values\n",
    "df_test_value.plot(legend=False, ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences from test values.\n",
    "# same as above rearrange the data frame \n",
    "# window size = 288\n",
    "x_test = windows(df_test_value.values)\n",
    "# check the x shape as output\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the dataset into the keras.Sequential model\n",
    "x_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "test_mae_loss = np.mean(np.abs(x_test_pred - x_test), axis=1)\n",
    "\n",
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "test_mae_loss = test_mae_loss.reshape((-1))\n",
    "\n",
    "test_mae_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pandas fig size (histogram)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot the histogram\n",
    "ax.hist(test_mae_loss, bins=50)\n",
    "\n",
    "# set x label\n",
    "ax.set_xlabel(\"test MAE loss\")\n",
    "\n",
    "# set y label\n",
    "ax.set_ylabel(\"No of samples\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect all the samples which are anomalies.\n",
    "anomalies = test_mae_loss > threshold\n",
    "\n",
    "# Number of anomalies.\n",
    "np.sum(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices of anomalies\n",
    "# DONT BOTHER DOING THIS\n",
    "np.where(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original code from url\n",
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "#anomalous_data_indices =[]\n",
    "#for data_idx in range(TIME_STEPS-1, len(df_test_value) - TIME_STEPS +1):\n",
    "    #if np.all(anomalies[data_idx -TIME_STEPS +1 : data_idx]):\n",
    "        #anomalous_data_indices.append(data_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplified\n",
    "# data i is an anomaly if samples [(i - timesteps + 1) to (i)] are anomalies\n",
    "inds = []\n",
    "\n",
    "for i in range(window_size - 1, len(df_test_value) - window_size + 1):\n",
    "    if np.all(anomalies[i - window_size + 1 : i]):\n",
    "        inds.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gest the subset of df_subset where anomaliess\n",
    "df_subset = df_daily_jumpsup.iloc[inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pandas plot size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# plot the original dataset in blue\n",
    "df_daily_jumpsup.plot(legend=False, ax=ax)\n",
    "\n",
    "# plot anomalies over the original plot in red\n",
    "df_subset.plot(legend=False, ax=ax, color=\"r\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
